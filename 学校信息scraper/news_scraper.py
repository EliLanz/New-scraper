#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æŠ“å–å·¥å…·
ä½¿ç”¨ newspaper3k æŠ“å–æ–°é—»å¹¶æŒ‰å…³é”®è¯åˆ†ç±»
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
import newspaper
from newspaper import Article, Source
import time
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsScraperError(Exception):
    """æ–°é—»æŠ“å–å¼‚å¸¸"""
    pass


class NewsScraper:
    """æ–°é—»æŠ“å–å™¨"""
    
    def __init__(self, config_path: str = "config.json"):
        """
        åˆå§‹åŒ–æ–°é—»æŠ“å–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.news_sources = self.config.get("news_sources", {})
        self.keywords = self.config.get("keywords", {})
        self.max_articles = self.config.get("max_articles", 50)
        self.language = self.config.get("language", "zh")
        
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            raise NewsScraperError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise NewsScraperError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def _match_keywords(self, text: str, category: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æŒ‡å®šåˆ†ç±»çš„å…³é”®è¯
        
        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
            category: å…³é”®è¯åˆ†ç±»
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        if not text or category not in self.keywords:
            return False
        
        text_lower = text.lower()
        keywords = self.keywords[category]
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return True
        return False
    
    def _fetch_articles_from_source(self, url: str) -> List[Article]:
        """
        ä»å•ä¸ªæ–°é—»æºæŠ“å–æ–‡ç« 
        
        Args:
            url: æ–°é—»æºURL
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = []
        try:
            logger.info(f"æ­£åœ¨æŠ“å–: {url}")
            
            # åˆ›å»ºæ–°é—»æº
            source = Source(url, language=self.language)
            source.build()
            
            # é™åˆ¶æ–‡ç« æ•°é‡
            article_urls = source.article_urls()[:self.max_articles]
            logger.info(f"å‘ç° {len(article_urls)} ç¯‡æ–‡ç« ")
            
            # æŠ“å–æ¯ç¯‡æ–‡ç« 
            for article_url in article_urls:
                try:
                    article = Article(article_url, language=self.language)
                    article.download()
                    article.parse()
                    articles.append(article)
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                except Exception as e:
                    logger.warning(f"æŠ“å–æ–‡ç« å¤±è´¥ {article_url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"æŠ“å–æ–°é—»æºå¤±è´¥ {url}: {e}")
        
        return articles
    
    def scrape_news(self) -> Dict[str, List[Dict]]:
        """
        æŠ“å–æ‰€æœ‰æ–°é—»æºçš„æ–°é—»
        
        Returns:
            æŒ‰åˆ†ç±»ç»„ç»‡çš„æ–°é—»å­—å…¸
        """
        categorized_news = {category: [] for category in self.keywords.keys()}
        categorized_news["å…¶ä»–"] = []
        
        # éå†æ‰€æœ‰æ–°é—»æº
        for region, sources in self.news_sources.items():
            logger.info(f"\nå¼€å§‹æŠ“å–ã€{region}ã€‘æ–°é—»æº")
            
            for source_url in sources:
                articles = self._fetch_articles_from_source(source_url)
                
                # åˆ†ç±»æ–‡ç« 
                for article in articles:
                    if not article.title:
                        continue
                    
                    article_info = {
                        "title": article.title,
                        "url": article.url,
                        "source": source_url,
                        "region": region,
                        "publish_date": article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥"
                    }
                    
                    # æ£€æŸ¥æ–‡ç« æ ‡é¢˜æ˜¯å¦åŒ¹é…å…³é”®è¯
                    matched = False
                    for category in self.keywords.keys():
                        if self._match_keywords(article.title, category):
                            categorized_news[category].append(article_info)
                            matched = True
                            break
                    
                    # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•åˆ†ç±»ï¼Œæ”¾å…¥"å…¶ä»–"
                    if not matched:
                        categorized_news["å…¶ä»–"].append(article_info)
        
        return categorized_news
    
    def generate_markdown_report(self, news_data: Dict[str, List[Dict]], output_dir: str = "output") -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„æ—¥æŠ¥
        
        Args:
            news_data: åˆ†ç±»åçš„æ–°é—»æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"news_daily_report_{today}.md"
        filepath = output_path / filename
        
        # ç”Ÿæˆ Markdown å†…å®¹
        content = []
        content.append(f"# æ–°é—»æ—¥æŠ¥ - {today}\n")
        content.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        content.append("---\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(articles) for articles in news_data.values())
        content.append(f"## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n")
        content.append(f"- **æ€»è®¡æ–‡ç« æ•°**: {total_articles}\n")
        for category, articles in news_data.items():
            if articles:
                content.append(f"- **{category}**: {len(articles)} ç¯‡\n")
        content.append("\n---\n")
        
        # æŒ‰åˆ†ç±»è¾“å‡ºæ–°é—»
        for category, articles in news_data.items():
            if not articles:
                continue
                
            content.append(f"\n## ğŸ“° {category}\n")
            content.append(f"*å…± {len(articles)} ç¯‡æ–°é—»*\n\n")
            
            for idx, article in enumerate(articles, 1):
                content.append(f"### {idx}. {article['title']}\n")
                content.append(f"- **é“¾æ¥**: [{article['url']}]({article['url']})\n")
                content.append(f"- **æ¥æº**: {article['source']}\n")
                content.append(f"- **åŒºåŸŸ**: {article['region']}\n")
                content.append(f"- **å‘å¸ƒæ—¥æœŸ**: {article['publish_date']}\n\n")
        
        # é¡µè„š
        content.append("\n---\n")
        content.append(f"*æŠ¥å‘Šç”±æ–°é—»æŠ“å–å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*\n")
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(''.join(content))
        
        logger.info(f"æ—¥æŠ¥å·²ç”Ÿæˆ: {filepath}")
        return str(filepath)
    
    def run(self, output_dir: str = "output") -> str:
        """
        è¿è¡Œæ–°é—»æŠ“å–å¹¶ç”ŸæˆæŠ¥å‘Š
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æŠ“å–æ–°é—»...")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # æŠ“å–æ–°é—»
        news_data = self.scrape_news()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = self.generate_markdown_report(news_data, output_dir)
        
        elapsed_time = time.time() - start_time
        logger.info("=" * 60)
        logger.info(f"æŠ“å–å®Œæˆ! è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"æŠ¥å‘Šè·¯å¾„: {report_path}")
        logger.info("=" * 60)
        
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    try:
        scraper = NewsScraper("config.json")
        report_path = scraper.run("output")
        print(f"\nâœ… æ–°é—»æ—¥æŠ¥ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {report_path}")
    except NewsScraperError as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    except KeyboardInterrupt:
        logger.info("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
