#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æŠ“å–å·¥å…· - é›†æˆ NewsAPI ç‰ˆæœ¬
æ”¯æŒ newspaper3k å’Œ NewsAPI ä¸¤ç§æŠ“å–æ–¹å¼
"""
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Optional
import time
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsScraperError(Exception):
    """æ–°é—»æŠ“å–å¼‚å¸¸"""
    pass


class NewsAPIIntegration:
    """NewsAPI é›†æˆ"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ– NewsAPI
        
        Args:
            api_key: NewsAPI å¯†é’¥
        """
        self.api_key = api_key
        self.base_url = "https://newsapi.org/v2"
        
        try:
            import requests
            self.requests = requests
        except ImportError:
            raise NewsScraperError("éœ€è¦å®‰è£… requests åº“: pip install requests")
    
    def get_top_headlines(self, 
                         category: Optional[str] = None,
                         country: str = 'us',
                         q: Optional[str] = None,
                         page_size: int = 100) -> List[Dict]:
        """
        è·å–å¤´æ¡æ–°é—»
        
        Args:
            category: åˆ†ç±» (business, entertainment, general, health, science, sports, technology)
            country: å›½å®¶ä»£ç  (us, cn, gb, etc.)
            q: æœç´¢å…³é”®è¯
            page_size: è¿”å›ç»“æœæ•°é‡ (æœ€å¤š100)
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        url = f"{self.base_url}/top-headlines"
        params = {
            'apiKey': self.api_key,
            'pageSize': min(page_size, 100)
        }
        
        if category:
            params['category'] = category
        if country:
            params['country'] = country
        if q:
            params['q'] = q
        
        try:
            response = self.requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') == 'ok':
                return data.get('articles', [])
            else:
                logger.error(f"NewsAPI é”™è¯¯: {data.get('message', 'Unknown error')}")
                return []
                
        except Exception as e:
            logger.error(f"NewsAPI è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def search_everything(self,
                         q: str,
                         from_date: Optional[datetime] = None,
                         to_date: Optional[datetime] = None,
                         language: str = 'en',
                         sort_by: str = 'publishedAt',
                         page_size: int = 100) -> List[Dict]:
        """
        æœç´¢æ‰€æœ‰æ–°é—»
        
        Args:
            q: æœç´¢å…³é”®è¯
            from_date: å¼€å§‹æ—¥æœŸ
            to_date: ç»“æŸæ—¥æœŸ
            language: è¯­è¨€ (ar, de, en, es, fr, he, it, nl, no, pt, ru, sv, ud, zh)
            sort_by: æ’åºæ–¹å¼ (relevancy, popularity, publishedAt)
            page_size: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        url = f"{self.base_url}/everything"
        params = {
            'apiKey': self.api_key,
            'q': q,
            'language': language,
            'sortBy': sort_by,
            'pageSize': min(page_size, 100)
        }
        
        if from_date:
            params['from'] = from_date.strftime('%Y-%m-%d')
        if to_date:
            params['to'] = to_date.strftime('%Y-%m-%d')
        
        try:
            response = self.requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') == 'ok':
                return data.get('articles', [])
            else:
                logger.error(f"NewsAPI é”™è¯¯: {data.get('message', 'Unknown error')}")
                return []
                
        except Exception as e:
            logger.error(f"NewsAPI è¯·æ±‚å¤±è´¥: {e}")
            return []


class NewspaperIntegration:
    """Newspaper3k é›†æˆ"""
    
    def __init__(self, language: str = 'en'):
        """
        åˆå§‹åŒ– Newspaper3k
        
        Args:
            language: è¯­è¨€ä»£ç 
        """
        self.language = language
        
        try:
            from newspaper import Article, Source
            self.Article = Article
            self.Source = Source
        except ImportError:
            raise NewsScraperError("éœ€è¦å®‰è£… newspaper3k åº“: pip install newspaper3k")
    
    def fetch_from_source(self, url: str, max_articles: int = 50) -> List[Dict]:
        """
        ä»æ–°é—»æºæŠ“å–æ–‡ç« 
        
        Args:
            url: æ–°é—»æº URL
            max_articles: æœ€å¤šæŠ“å–æ–‡ç« æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = []
        try:
            logger.info(f"æ­£åœ¨æŠ“å–: {url}")
            
            source = self.Source(url, language=self.language)
            source.build()
            
            article_urls = source.article_urls()[:max_articles]
            logger.info(f"å‘ç° {len(article_urls)} ç¯‡æ–‡ç« ")
            
            for article_url in article_urls:
                try:
                    article = self.Article(article_url, language=self.language)
                    article.download()
                    article.parse()
                    
                    articles.append({
                        'title': article.title,
                        'url': article.url,
                        'source': url,
                        'publish_date': article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥",
                        'description': article.text[:200] + '...' if article.text else ''
                    })
                    
                    time.sleep(0.5)
                except Exception as e:
                    logger.warning(f"æŠ“å–æ–‡ç« å¤±è´¥ {article_url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"æŠ“å–æ–°é—»æºå¤±è´¥ {url}: {e}")
        
        return articles


class DeepSeekSummarizer:
    """DeepSeek å¤§è¯­è¨€æ¨¡å‹é›†æˆ - ç”¨äºç”Ÿæˆæ–°é—»æ‘˜è¦"""
    
    def __init__(self, api_key: str, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ– DeepSeek
        
        Args:
            api_key: DeepSeek API å¯†é’¥
            config: æ‘˜è¦é…ç½®
        """
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com"
        self.config = config or {}
        
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=api_key,
                base_url=self.base_url
            )
            logger.info("âœ… DeepSeek å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            raise NewsScraperError("éœ€è¦å®‰è£… openai åº“: pip install openai")
        except Exception as e:
            raise NewsScraperError(f"DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def generate_daily_summary(self, news_data: Dict[str, List[Dict]]) -> str:
        """
        ç”Ÿæˆå½“æ—¥æ–°é—»æ‘˜è¦
        
        Args:
            news_data: åˆ†ç±»åçš„æ–°é—»æ•°æ®
            
        Returns:
            ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        # å‡†å¤‡æ–°é—»æ ‡é¢˜æ•°æ®
        news_text = self._prepare_news_text(news_data)
        
        if not news_text.strip():
            logger.warning("æ²¡æœ‰æ–°é—»æ•°æ®ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            return ""
        
        # æ„å»ºæç¤ºè¯
        min_length = self.config.get('min_length', 300)
        max_length = self.config.get('max_length', 500)
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä»Šæ—¥æ–°é—»æ ‡é¢˜ï¼Œç”Ÿæˆä¸€æ®µ{min_length}-{max_length}å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æ‘˜è¦åº”å…·æœ‰æ•´ä½“æ„Ÿï¼Œèƒ½å¤Ÿæç‚¼å‡ºå½“æ—¥æ–°é—»çš„ä¸»è¦è¶‹åŠ¿ã€å…³æ³¨ç„¦ç‚¹æˆ–èˆ†è®ºåŠ¨å‘
2. ä¸è¦æœºæ¢°å¤è¿°æ ‡é¢˜ï¼Œè¦è¿›è¡Œæ¦‚æ‹¬ä¸ä¸²è”ï¼Œé£æ ¼è‡ªç„¶æµç•…
3. åˆ†æå„ç±»æ–°é—»çš„å†…åœ¨è”ç³»å’Œæ•´ä½“è¶‹åŠ¿
4. è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé€‚åˆå¿«é€Ÿé˜…è¯»

ä»Šæ—¥æ–°é—»æ ‡é¢˜ï¼š

{news_text}

è¯·ç”Ÿæˆæ‘˜è¦ï¼š"""
        
        try:
            logger.info("ğŸ¤– æ­£åœ¨è°ƒç”¨ DeepSeek ç”Ÿæˆæ‘˜è¦...")
            
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ–°é—»åˆ†æå¸ˆå’Œç¼–è¾‘ï¼Œæ“…é•¿ä»å¤§é‡æ–°é—»ä¸­æç‚¼å…³é”®ä¿¡æ¯ï¼Œå‘ç°è¶‹åŠ¿å’Œè”ç³»ï¼Œç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€æ’°å†™å¯¼è¯»æ‘˜è¦ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=False
            )
            
            summary = response.choices[0].message.content.strip()
            logger.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)} å­—")
            
            return summary
            
        except Exception as e:
            logger.error(f"DeepSeek æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _prepare_news_text(self, news_data: Dict[str, List[Dict]]) -> str:
        """
        å‡†å¤‡ç”¨äºç”Ÿæˆæ‘˜è¦çš„æ–°é—»æ–‡æœ¬
        
        Args:
            news_data: åˆ†ç±»åçš„æ–°é—»æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„æ–°é—»æ–‡æœ¬
        """
        text_parts = []
        
        for category, articles in news_data.items():
            if not articles or category == "å…¶ä»–":
                continue
            
            text_parts.append(f"\nã€{category}ç±»ã€‘({len(articles)}ç¯‡)")
            
            # æ¯ä¸ªåˆ†ç±»æœ€å¤šå–å‰10æ¡
            for idx, article in enumerate(articles[:10], 1):
                title = article.get('title', '').strip()
                if title:
                    text_parts.append(f"{idx}. {title}")
        
        return '\n'.join(text_parts)


class HybridNewsScraper:
    """æ··åˆæ–°é—»æŠ“å–å™¨ - æ”¯æŒ NewsAPI å’Œ Newspaper3k"""
    
    def __init__(self, config_path: str = "config.json"):
        """
        åˆå§‹åŒ–æ··åˆæ–°é—»æŠ“å–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # åŸºç¡€é…ç½®
        self.keywords = self.config.get("keywords", {})
        self.language = self.config.get("language", "zh")
        
        # NewsAPI é…ç½®
        self.use_newsapi = self.config.get("use_newsapi", False)
        self.newsapi_key = self.config.get("newsapi_key", "")
        self.newsapi = None
        
        if self.use_newsapi and self.newsapi_key:
            try:
                self.newsapi = NewsAPIIntegration(self.newsapi_key)
                logger.info("âœ… NewsAPI å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"NewsAPI åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_newsapi = False
        
        # Newspaper3k é…ç½®
        self.use_newspaper = self.config.get("use_newspaper", True)
        self.news_sources = self.config.get("news_sources", {})
        self.max_articles = self.config.get("max_articles", 50)
        self.newspaper = None
        
        if self.use_newspaper:
            try:
                self.newspaper = NewspaperIntegration(self.language)
                logger.info("âœ… Newspaper3k å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"Newspaper3k åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_newspaper = False
        
        # DeepSeek é…ç½®
        self.use_deepseek = self.config.get("use_deepseek_summary", False)
        self.deepseek_api_key = self.config.get("deepseek_api_key", "")
        self.deepseek = None
        
        if self.use_deepseek and self.deepseek_api_key:
            try:
                deepseek_config = self.config.get("deepseek_summary_config", {})
                self.deepseek = DeepSeekSummarizer(self.deepseek_api_key, deepseek_config)
                logger.info("âœ… DeepSeek æ‘˜è¦å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"DeepSeek åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_deepseek = False
    
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            raise NewsScraperError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise NewsScraperError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def _match_keywords(self, text: str, category: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æŒ‡å®šåˆ†ç±»çš„å…³é”®è¯"""
        if not text or category not in self.keywords:
            return False
        
        text_lower = text.lower()
        keywords = self.keywords[category]
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return True
        return False
    
    def _categorize_article(self, article: Dict) -> str:
        """
        ä¸ºæ–‡ç« åˆ†ç±»
        
        Args:
            article: æ–‡ç« ä¿¡æ¯
            
        Returns:
            åˆ†ç±»åç§°
        """
        title = article.get('title', '')
        description = article.get('description', '')
        text = f"{title} {description}"
        
        for category in self.keywords.keys():
            if self._match_keywords(text, category):
                return category
        
        return "å…¶ä»–"
    
    def scrape_from_newsapi(self) -> List[Dict]:
        """ä» NewsAPI æŠ“å–æ–°é—»"""
        if not self.newsapi:
            return []
        
        all_articles = []
        newsapi_config = self.config.get("newsapi_config", {})
        
        # è·å–å¤´æ¡æ–°é—»
        if newsapi_config.get("fetch_top_headlines", True):
            logger.info("\nğŸ“° ä» NewsAPI è·å–å¤´æ¡æ–°é—»...")
            
            countries = newsapi_config.get("countries", ["us"])
            categories = newsapi_config.get("categories", ["technology", "business", "science"])
            
            for country in countries:
                for category in categories:
                    logger.info(f"  è·å– {country} - {category}")
                    articles = self.newsapi.get_top_headlines(
                        category=category,
                        country=country,
                        page_size=self.max_articles
                    )
                    
                    for article in articles:
                        all_articles.append({
                            'title': article.get('title', ''),
                            'url': article.get('url', ''),
                            'source': article.get('source', {}).get('name', 'NewsAPI'),
                            'region': f"NewsAPI-{country}",
                            'publish_date': article.get('publishedAt', '')[:10] if article.get('publishedAt') else 'æœªçŸ¥',
                            'description': article.get('description', '')
                        })
                    
                    time.sleep(0.5)
        
        # æŒ‰å…³é”®è¯æœç´¢
        if newsapi_config.get("search_by_keywords", True):
            logger.info("\nğŸ” ä» NewsAPI æŒ‰å…³é”®è¯æœç´¢...")
            
            from_date = datetime.now() - timedelta(days=newsapi_config.get("days_back", 7))
            language = newsapi_config.get("search_language", "en")
            
            for category, keywords in self.keywords.items():
                # å–å‰3ä¸ªå…³é”®è¯æœç´¢
                search_keywords = keywords[:3]
                for keyword in search_keywords:
                    logger.info(f"  æœç´¢: {keyword}")
                    articles = self.newsapi.search_everything(
                        q=keyword,
                        from_date=from_date,
                        language=language,
                        page_size=20
                    )
                    
                    for article in articles:
                        all_articles.append({
                            'title': article.get('title', ''),
                            'url': article.get('url', ''),
                            'source': article.get('source', {}).get('name', 'NewsAPI'),
                            'region': f"NewsAPI-æœç´¢",
                            'publish_date': article.get('publishedAt', '')[:10] if article.get('publishedAt') else 'æœªçŸ¥',
                            'description': article.get('description', '')
                        })
                    
                    time.sleep(0.5)
        
        logger.info(f"âœ… NewsAPI å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def scrape_from_newspaper(self) -> List[Dict]:
        """ä» Newspaper3k æŠ“å–æ–°é—»"""
        if not self.newspaper:
            return []
        
        all_articles = []
        
        logger.info("\nğŸ“° ä½¿ç”¨ Newspaper3k æŠ“å–æ–°é—»æº...")
        
        for region, sources in self.news_sources.items():
            logger.info(f"\nå¼€å§‹æŠ“å–ã€{region}ã€‘æ–°é—»æº")
            
            for source_url in sources:
                articles = self.newspaper.fetch_from_source(source_url, self.max_articles)
                
                for article in articles:
                    article['region'] = region
                    all_articles.append(article)
        
        logger.info(f"âœ… Newspaper3k å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def scrape_news(self) -> Dict[str, List[Dict]]:
        """
        æŠ“å–æ‰€æœ‰æ–°é—»æºçš„æ–°é—»
        
        Returns:
            æŒ‰åˆ†ç±»ç»„ç»‡çš„æ–°é—»å­—å…¸
        """
        all_articles = []
        
        # ä» NewsAPI æŠ“å–
        if self.use_newsapi:
            all_articles.extend(self.scrape_from_newsapi())
        
        # ä» Newspaper3k æŠ“å–
        if self.use_newspaper:
            all_articles.extend(self.scrape_from_newspaper())
        
        # å»é‡ï¼ˆåŸºäº URLï¼‰
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            url = article.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        logger.info(f"\nğŸ“Š å»é‡åå…± {len(unique_articles)} ç¯‡æ–‡ç« ")
        
        # åˆ†ç±»
        categorized_news = {category: [] for category in self.keywords.keys()}
        categorized_news["å…¶ä»–"] = []
        
        for article in unique_articles:
            category = self._categorize_article(article)
            categorized_news[category].append(article)
        
        return categorized_news
    
    def generate_markdown_report(self, news_data: Dict[str, List[Dict]], output_dir: str = "output") -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼çš„æ—¥æŠ¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"news_daily_report_{today}.md"
        filepath = output_path / filename
        
        content = []
        content.append(f"# æ–°é—»æ—¥æŠ¥ - {today}\n")
        content.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # æ•°æ®æºè¯´æ˜
        sources_used = []
        if self.use_newsapi:
            sources_used.append("NewsAPI")
        if self.use_newspaper:
            sources_used.append("Newspaper3k")
        if self.use_deepseek:
            sources_used.append("DeepSeek AI æ‘˜è¦")
        content.append(f"> æ•°æ®æº: {', '.join(sources_used)}\n")
        content.append("---\n")
        
        # AI ç”Ÿæˆçš„ä»Šæ—¥å¯¼è§ˆæ‘˜è¦
        if self.use_deepseek and self.deepseek:
            logger.info("\nğŸ¤– ç”Ÿæˆ AI æ‘˜è¦...")
            summary = self.deepseek.generate_daily_summary(news_data)
            
            if summary:
                content.append("\n## ğŸŒŸ ä»Šæ—¥å¯¼è§ˆæ‘˜è¦\n\n")
                content.append("> *ç”± DeepSeek AI æ ¹æ®å½“æ—¥æ–°é—»è‡ªåŠ¨ç”Ÿæˆ*\n\n")
                content.append(f"{summary}\n\n")
                content.append("---\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(articles) for articles in news_data.values())
        content.append(f"\n## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n")
        content.append(f"- **æ€»è®¡æ–‡ç« æ•°**: {total_articles}\n")
        for category, articles in news_data.items():
            if articles:
                content.append(f"- **{category}**: {len(articles)} ç¯‡\n")
        content.append("\n---\n")
        
        # æŒ‰åˆ†ç±»è¾“å‡ºæ–°é—»
        for category, articles in news_data.items():
            if not articles:
                continue
                
            content.append(f"\n## ğŸ“° {category}\n")
            content.append(f"*å…± {len(articles)} ç¯‡æ–°é—»*\n\n")
            
            for idx, article in enumerate(articles, 1):
                content.append(f"### {idx}. {article['title']}\n")
                content.append(f"- **é“¾æ¥**: [{article['url']}]({article['url']})\n")
                content.append(f"- **æ¥æº**: {article['source']}\n")
                content.append(f"- **åŒºåŸŸ**: {article['region']}\n")
                content.append(f"- **å‘å¸ƒæ—¥æœŸ**: {article['publish_date']}\n")
                
                if article.get('description'):
                    content.append(f"- **æ‘˜è¦**: {article['description'][:150]}...\n")
                
                content.append("\n")
        
        # é¡µè„š
        content.append("\n---\n")
        content.append(f"*æŠ¥å‘Šç”±æ–°é—»æŠ“å–å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*\n")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(''.join(content))
        
        logger.info(f"æ—¥æŠ¥å·²ç”Ÿæˆ: {filepath}")
        return str(filepath)
    
    def run(self, output_dir: str = "output") -> str:
        """è¿è¡Œæ–°é—»æŠ“å–å¹¶ç”ŸæˆæŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹æŠ“å–æ–°é—»...")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        news_data = self.scrape_news()
        report_path = self.generate_markdown_report(news_data, output_dir)
        
        elapsed_time = time.time() - start_time
        logger.info("=" * 60)
        logger.info(f"æŠ“å–å®Œæˆ! è€—æ—¶: {elapsed_time:.2f} ç§’")
        logger.info(f"æŠ¥å‘Šè·¯å¾„: {report_path}")
        logger.info("=" * 60)
        
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    try:
        scraper = HybridNewsScraper("config.json")
        report_path = scraper.run("output")
        print(f"\nâœ… æ–°é—»æ—¥æŠ¥ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {report_path}")
    except NewsScraperError as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    except KeyboardInterrupt:
        logger.info("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
